# Teams AI Summarizer

AI-powered summarization for Microsoft Teams **chats** (1:1 and group conversations) with **dual LLM support** - choose between local Ollama (free, private) or Claude API (cloud, higher quality).

## âœ… Status: Code Complete

All features implemented and tested! The application is ready for development use with mock data or production use with Azure AD.

## ğŸ¯ Latest Update: Dual LLM Provider Support

**NEW**: Switch between Ollama and Claude API instantly with a UI toggle - no server restart required!

## Features

âœ… **Dual LLM Providers** - Switch between Ollama (local) and Claude API (cloud) with one click
âœ… **Live Provider Toggle** - Change AI providers instantly via UI dropdown
âœ… **Authentication** - Mock mode (for development) or Azure AD (for production)
âœ… **Ollama Playground** - Direct chat interface with local LLM
âœ… **Claude Playground** - Direct chat interface with Claude API
âœ… **Chat Management** - Browse and select Teams chats to monitor (filtered to last 7 days, max 50)
âœ… **Message Viewer** - View and refresh messages from monitored chats
âœ… **AI Summarization** - Generate structured summaries with your chosen provider
âœ… **Smart Caching** - SQLite database for messages and summaries
âœ… **TypeScript** - Fully typed with strict mode
âœ… **Tests** - 101 tests passing with 85%+ coverage

## Quick Start

### Choose Your AI Provider

The app supports two AI providers:

| Provider | Cost | Speed | Quality | Privacy |
|----------|------|-------|---------|---------|
| **Ollama** | Free | 5-30s | Good | 100% local |
| **Claude API** | ~$15-60/mo | 2-5s | Excellent | Cloud (Anthropic) |

**Recommendation**: Start with Ollama (free) to test, upgrade to Claude for higher quality.

### Setup with Ollama (Free, Local)

1. **Install Ollama** and pull a model:
   ```bash
   curl -fsSL https://ollama.com/install.sh | sh
   ollama pull llama3
   ollama serve  # Keep running in a terminal
   ```

2. **Install dependencies**:
   ```bash
   npm install
   ```

3. **Configure environment** (`.env.local` already configured):
   ```bash
   AI_PROVIDER=ollama
   NEXT_PUBLIC_AI_PROVIDER=ollama
   OLLAMA_BASE_URL=http://localhost:11434
   OLLAMA_MODEL=llama3
   ```

4. **Run development server**:
   ```bash
   npm run dev
   ```

5. **Visit http://localhost:3000**
   - Click "Sign in (Mock)"
   - See Ollama Playground (purple theme)
   - Add chats, view messages, generate summaries

### Setup with Claude API (Paid, Cloud)

1. **Get Claude API key**:
   - Visit [console.anthropic.com](https://console.anthropic.com)
   - Sign up and create an API key
   - Add payment method

2. **Configure environment** (edit `.env.local`):
   ```bash
   AI_PROVIDER=claude
   NEXT_PUBLIC_AI_PROVIDER=claude
   ANTHROPIC_API_KEY=sk-ant-your-api-key-here
   CLAUDE_MODEL=claude-sonnet-4-20250514
   ```

3. **Run development server**:
   ```bash
   npm run dev
   ```

4. **Visit http://localhost:3000**
   - Click "Sign in (Mock)"
   - See Claude Playground (blue theme)
   - Generate summaries with Claude API

### Switch Providers Instantly

**No restart required!** Use the dropdown in the dashboard header:

1. Select **ğŸŸ£ Ollama (Local)** â†’ Uses local Ollama
2. Select **ğŸ”µ Claude API** â†’ Uses cloud Claude
3. Selection persists in browser localStorage
4. Each summary uses your selected provider

## Documentation

- **[QUICK_START.md](QUICK_START.md)** - Get started in 5 minutes
- **[docs/claude-migration.md](docs/claude-migration.md)** - Switching between providers â­ NEW
- **[MOCK_MODE_GUIDE.md](MOCK_MODE_GUIDE.md)** - Using mock authentication
- **[PRODUCTION.md](PRODUCTION.md)** - Production deployment with Azure AD
- **[CLAUDE.md](CLAUDE.md)** - Developer guidance for this codebase
- **[docs/setup.md](docs/setup.md)** - Complete setup instructions
- **[docs/architecture.md](docs/architecture.md)** - System design and database schema
- **[docs/api-integrations.md](docs/api-integrations.md)** - LLM and Graph API details

## Development Commands

```bash
npm run dev           # Start development server (http://localhost:3000)
npm test              # Run tests in watch mode
npm run test:ci       # Run tests once (for CI)
npm run test:coverage # Run tests with coverage report
npm run lint          # Run ESLint
npm run format        # Format code with Prettier
npm run build         # Build for production
npm start             # Start production server
```

## Project Structure

```
teams-summerize/
â”œâ”€â”€ app/                    # Next.js App Router
â”‚   â”œâ”€â”€ api/               # API routes
â”‚   â”‚   â”œâ”€â”€ auth/[...nextauth]/  # Authentication (NextAuth)
â”‚   â”‚   â”œâ”€â”€ chats/               # Chat management
â”‚   â”‚   â”œâ”€â”€ claude/chat/         # Claude playground endpoint â­ NEW
â”‚   â”‚   â”œâ”€â”€ messages/            # Message fetching
â”‚   â”‚   â””â”€â”€ summarize/           # AI summarization (dual provider)
â”‚   â”œâ”€â”€ page.tsx           # Main dashboard with provider toggle
â”‚   â”œâ”€â”€ layout.tsx         # Root layout
â”‚   â””â”€â”€ providers.tsx      # Client-side providers (incl. ProviderContext)
â”œâ”€â”€ components/            # React components
â”‚   â”œâ”€â”€ ChannelSelector.tsx     # Channel selection UI
â”‚   â”œâ”€â”€ MessageViewer.tsx       # Message display UI
â”‚   â”œâ”€â”€ SummarizePanel.tsx      # AI summarization UI
â”‚   â”œâ”€â”€ OllamaPlayground.tsx    # Direct Ollama chat
â”‚   â”œâ”€â”€ ClaudePlayground.tsx    # Direct Claude chat â­ NEW
â”‚   â””â”€â”€ ProviderToggle.tsx      # Provider selector dropdown â­ NEW
â”œâ”€â”€ contexts/              # React contexts â­ NEW
â”‚   â””â”€â”€ ProviderContext.tsx     # AI provider state management
â”œâ”€â”€ lib/                   # Core business logic
â”‚   â”œâ”€â”€ auth.ts           # Authentication utilities
â”‚   â”œâ”€â”€ db.ts             # Database operations (SQLite)
â”‚   â”œâ”€â”€ errors.ts         # Custom error classes (incl. ClaudeAPIError)
â”‚   â”œâ”€â”€ llm-provider.ts   # LLM provider abstraction â­ NEW
â”‚   â”œâ”€â”€ claude.ts         # Claude API integration â­ NEW
â”‚   â”œâ”€â”€ ollama.ts         # Ollama integration
â”‚   â”œâ”€â”€ microsoft-graph.ts # Graph API client (with mock support)
â”‚   â”œâ”€â”€ validation.ts     # Input validation (Zod)
â”‚   â””â”€â”€ mock-data.ts      # Sample data for development
â”œâ”€â”€ __tests__/            # Jest tests (101 passing) â­ UPDATED
â”‚   â”œâ”€â”€ lib/claude.test.ts       # Claude integration tests â­ NEW
â”‚   â””â”€â”€ lib/llm-provider.test.ts # Provider factory tests â­ NEW
â”œâ”€â”€ data/                 # SQLite database (auto-created)
â””â”€â”€ docs/                 # Detailed documentation
    â””â”€â”€ claude-migration.md      # Provider migration guide â­ NEW
```

## Tech Stack

- **Frontend**: Next.js 14 (App Router), React 18, Tailwind CSS
- **Backend**: Next.js API Routes
- **Database**: SQLite (better-sqlite3)
- **AI**: Dual LLM support
  - **Ollama** - Local LLM (llama3/mistral) - Free, private
  - **Claude API** - Anthropic's Claude Sonnet 4 - Paid, cloud
- **Auth**: NextAuth with Azure AD or Mock provider
- **Testing**: Jest, React Testing Library (101 tests, 85%+ coverage)
- **Code Quality**: ESLint, Prettier, TypeScript (strict mode)

## Key Features Explained

### ğŸ¨ Live Provider Toggle
Switch between Ollama and Claude instantly with the dropdown in the dashboard header. Your choice is saved and persists across sessions.

### ğŸ® Dual Playgrounds
- **Ollama Playground** (purple) - Direct chat with local LLM
- **Claude Playground** (blue) - Direct chat with Claude API
- Both show performance metrics and token usage

### ğŸ“Š Chat Selector
Browse your Teams chats (1:1 and group) and select which ones to monitor. Smart filtering shows only chats with activity in the **last 7 days** (max 50), making selection fast and relevant.

### ğŸ’¬ Message Viewer
View messages from monitored chats with author names, timestamps, and formatted content. Refresh to fetch new messages.

### ğŸ¤– AI Summarization
Generate structured summaries with your chosen provider:
- ğŸ“‹ **Overview** - High-level summary of the conversation
- âœ… **Key Decisions** - Important decisions made
- ğŸ¯ **Action Items** - Tasks with @mentions for assignees
- ğŸš§ **Blockers** - Issues preventing progress
- ğŸ”— **Resources** - Links and references mentioned

### ğŸ’¾ Smart Caching
All messages are cached in SQLite to minimize API calls and improve performance.

### ğŸ“Š Cost Tracking
For Claude API users, token usage is logged with each request for accurate cost tracking.

## Testing

The project has comprehensive test coverage with **101 tests passing**:

```bash
# Run all tests
npm test

# Run specific test
npm test -- __tests__/lib/claude.test.ts

# Check coverage
npm run test:coverage
```

**Test Coverage**:
- `lib/claude.ts`: 100%
- `lib/llm-provider.ts`: 100%
- `lib/ollama.ts`: 96.61%
- Overall: 85%+ on critical paths

## Provider Comparison

### Ollama (Local)

**Pros**:
- âœ… Completely free
- âœ… 100% private (data never leaves your machine)
- âœ… Works offline
- âœ… No API key required

**Cons**:
- âŒ Requires 16GB+ RAM
- âŒ Slower (5-30s per summary)
- âŒ Slightly lower quality

**Best for**: Testing, high privacy requirements, cost-conscious users

### Claude API (Cloud)

**Pros**:
- âœ… Excellent summary quality
- âœ… Fast (2-5s per summary)
- âœ… No hardware requirements
- âœ… Larger context window (200k tokens)

**Cons**:
- âŒ Costs ~$15-60/month
- âŒ Requires internet
- âŒ Data sent to Anthropic's cloud

**Best for**: Production use, high-quality summaries, limited local resources

**Cost estimate**: ~$0.50-2/day for typical usage (5 monitored chats)

## Development vs Production

### Development Mode (Current)
- âœ… Mock authentication (no Azure AD needed)
- âœ… Sample Teams chats and messages
- âœ… Full dual LLM integration
- âœ… All features working
- âœ… Perfect for UI development and testing

### Production Mode
- ğŸ” Real Azure AD authentication
- ğŸ“¡ Real Microsoft Graph API calls
- ğŸ’¼ Your actual Teams chats and messages
- ğŸš€ See [PRODUCTION.md](PRODUCTION.md) for setup

## Common Tasks

### Test Ollama Connection
```bash
curl http://localhost:11434/api/tags
```

### Test Claude API
Visit the Claude Playground in the app and try a test prompt.

### View Database
```bash
sqlite3 data/app.db
SELECT * FROM monitored_chats;
SELECT COUNT(*) FROM messages;
SELECT * FROM summaries ORDER BY generated_at DESC LIMIT 5;
```

### Clear Database
```bash
rm data/app.db
# Will be recreated on next run
```

### Switch Providers
Edit `.env.local` and change `AI_PROVIDER=ollama` to `AI_PROVIDER=claude` (or vice versa), then restart:
```bash
npm run dev
```

Or use the UI toggle (no restart needed)!

## Troubleshooting

### Ollama Not Working
- Ensure Ollama is running: `ollama serve`
- Check model is installed: `ollama list`
- Verify URL in `.env.local`: `OLLAMA_BASE_URL=http://localhost:11434`

### Claude API Not Working
- Check API key is set in `.env.local`: `ANTHROPIC_API_KEY=sk-ant-...`
- Verify you have API credits at [console.anthropic.com](https://console.anthropic.com)
- Check internet connection

### Provider Toggle Not Appearing
- Ensure `NEXT_PUBLIC_AI_PROVIDER` is set in `.env.local`
- Restart dev server after changing environment variables
- Clear browser cache

### TypeScript Errors
- Clear cache: `rm -rf .next`
- Verify tsconfig.json has `"target": "es2018"`

### Mock Data Not Showing
- Check `.env.local` has `USE_MOCK_DATA=true`
- Restart dev server after changing environment variables

## Environment Variables

See `.env.local.example` for a complete list. Key variables:

```bash
# AI Provider Selection
AI_PROVIDER=ollama                      # 'ollama' or 'claude'
NEXT_PUBLIC_AI_PROVIDER=ollama          # For client-side display

# Ollama (if AI_PROVIDER=ollama)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3

# Claude API (if AI_PROVIDER=claude)
ANTHROPIC_API_KEY=sk-ant-your-key-here
CLAUDE_MODEL=claude-sonnet-4-20250514

# Mock mode (for development)
USE_MOCK_AUTH=true
USE_MOCK_DATA=true
NEXT_PUBLIC_USE_MOCK_AUTH=true
```

## License

Private project for personal use.

## Acknowledgments

Built with Claude Code following TDD principles with comprehensive error handling, type safety, and dual LLM provider support.

---

**Ready to try it?** Start with Ollama (free) and upgrade to Claude when you need higher quality summaries! ğŸš€
